{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Functional Programming [25 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a function add that takes an arbitrary number of arguments, and adds them all.\n",
    "Also create a function sub that subtracts all the arguments but the first from the first1.\n",
    "Also create a function ra_sub that performs right-associative subtraction\n",
    "add(1, 2, 3) => 6\n",
    "sub(5, 1, 2) => 2\n",
    "ra_sub(5, 1, 2) => (5 - (1 - 2)) => 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "2\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "# Function to add arbitrary number of arguments\n",
    "def add(*args):\n",
    "    return sum(args)\n",
    "\n",
    "# Function to subtract all arguments but the first from the first\n",
    "def sub(first, *args):\n",
    "    return first - sum(args)\n",
    "\n",
    "# Function to perform right-associative subtraction\n",
    "def ra_sub(*args):\n",
    "    return reduce(lambda x, y: y - x, reversed(args))\n",
    "\n",
    "# Test cases\n",
    "print(add(1, 2, 3))  # Output: 6\n",
    "print(sub(5, 1, 2))  # Output: 2\n",
    "print(ra_sub(5, 1, 2))  # Output: 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a function zip that takes an arbitrary number of sequences, and zips them, i.e.\n",
    "creates a list of lists, where the inner lists consist of the first elements from the given\n",
    "sequences, then the second elements from the given sequences, and so on.\n",
    "\n",
    "    zip([1, 2, 3], [4, 5, 6]) => [[1, 4], [2, 5], [3, 6]]\n",
    "\n",
    "    zip([1, 2, 3], [4, 5, 6], [7, 8, 9]) => [[1, 4, 7], [2, 5, 8], [3, 6, 9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 4], [2, 5], [3, 6]]\n",
      "[[1, 4, 7], [2, 5, 8], [3, 6, 9]]\n"
     ]
    }
   ],
   "source": [
    "# Function to zip multiple sequences into a list of lists\n",
    "def zip_lists(*sequences):\n",
    "    return list(map(list, zip(*sequences)))\n",
    "\n",
    "# Test cases\n",
    "print(zip_lists([1, 2, 3], [4, 5, 6]))  # Output: [[1, 4], [2, 5], [3, 6]]\n",
    "print(zip_lists([1, 2, 3], [4, 5, 6], [7, 8, 9]))  # Output: [[1, 4, 7], [2, 5, 8], [3, 6, 9]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create a function zipwith that takes a function f and an arbitrary number of sequences,\n",
    "and returns a list of f applied to the first elements of the given sequences, followed by f\n",
    "applied to the second elements of the sequences, and so on.\n",
    "\n",
    "    zipwith(add, [1, 2, 3], [4, 5, 6]) => [5, 7, 9]\n",
    "\n",
    "    zipwith(add, [1, 2, 3], [4, 5, 6], [1, 1, 1]) => [6, 8, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 7, 9]\n",
      "[6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "# Function to apply a given function on elements of multiple sequences\n",
    "def zipwith(f, *sequences):\n",
    "    return list(map(lambda *args: f(*args), *sequences))\n",
    "\n",
    "# Test cases\n",
    "print(zipwith(add, [1, 2, 3], [4, 5, 6]))  # Output: [5, 7, 9]\n",
    "print(zipwith(add, [1, 2, 3], [4, 5, 6], [1, 1, 1]))  # Output: [6, 8, 10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Create a function flatten that can flatten a tree.\n",
    "\n",
    "    flatten([1, [2, [3, 4], [5, 6], 7], 8, [9, 10]])\n",
    "=> [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "# Function to flatten nested lists (or trees)\n",
    "def flatten(lst):\n",
    "   return sum(map(flatten, lst), []) if isinstance(lst, list) else [lst]\n",
    "\n",
    "# Test case\n",
    "print(flatten([1, [2, [3, 4], [5, 6], 7], 8, [9, 10]]))  # Output: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Create a function group_by that takes a function and a sequence and groups the\n",
    "elements of the sequence based on the result of the given function. In the example\n",
    "below, len returns the length of a sequence.\n",
    "\n",
    "    group_by(len, [\"hi\", \"dog\", \"me\", \"bad\", \"good\"])\n",
    "=> {2: [\"hi\", \"me\"], 3: [\"dog\", \"bad\"], 4: [\"good\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2: ['hi', 'me'], 3: ['dog', 'bad'], 4: ['good']}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "\n",
    "# Function to group elements of a sequence based on a given function\n",
    "def group_by(f, sequence):\n",
    "    return reduce(lambda acc, item: {**acc, f(item): acc.get(f(item), []) + [item]}, sequence, defaultdict(list))\n",
    "\n",
    "# Test case\n",
    "print(group_by(len, [\"hi\", \"dog\", \"me\", \"bad\", \"good\"]))  # Output: {2: ['hi', 'me'], 3: ['dog', 'bad'], 4: ['good']}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Confirming Hadoop Installation [15 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [3 points] Acquire the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I followed step one in the big-data-repo github page belonging to Professor Singh. I acquired the cluster doing those steps plus disabling internal ip address option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. [3 points] Load the data into the master, move the data into HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I sshed into the cluster master and cloned the repo and then created a directory called /user/adityad and put the files from the cloned repo into the HDFS and then checked if the five-books are present there.\n",
    "\n",
    "Output:\n",
    "\n",
    "Found 5 items\n",
    "\n",
    "-rw-r--r--   1 aditya_duggirala hadoop     179903 2024-10-13 19:13 /user/adityad/five-books/a_tangled_tale.txt\n",
    "\n",
    "-rw-r--r--   1 aditya_duggirala hadoop     173379 2024-10-13 19:13 /user/adityad/five-books/alice_in_wonderland.txt\n",
    "\n",
    "-rw-r--r--   1 aditya_duggirala hadoop     394246 2024-10-13 19:13 /user/adityad/five-books/sylvie_and_bruno.txt\n",
    "\n",
    "-rw-r--r--   1 aditya_duggirala hadoop     458755 2024-10-13 19:13 /user/adityad/five-books/symbolic_logic.txt\n",
    "\n",
    "-rw-r--r--   1 aditya_duggirala hadoop     135443 2024-10-13 19:13 /user/adityad/five-books/the_game_of_logic.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. [3 points] Without writing any code of your own, verify that you have a good\n",
    "installation of hadoop by running wordcount on five-books. The command is\n",
    "similar to\n",
    "\n",
    "    hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount /user/singhj/five-books\n",
    "/books-count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran the command and this is the ouput I got:\n",
    "\n",
    "2024-10-13 22:23:30,287 INFO mapreduce.Job: Job job_1728858072744_0001 completed successfully\n",
    "2024-10-13 22:23:30,376 INFO mapreduce.Job: Counters: 55\n",
    "        File System Counters\n",
    "                FILE: Number of bytes read=596684\n",
    "                FILE: Number of bytes written=3493255\n",
    "                FILE: Number of read operations=0\n",
    "                FILE: Number of large read operations=0\n",
    "                FILE: Number of write operations=0\n",
    "                HDFS: Number of bytes read=1342381\n",
    "                HDFS: Number of bytes written=313829\n",
    "                HDFS: Number of read operations=30\n",
    "                HDFS: Number of large read operations=0\n",
    "                HDFS: Number of write operations=9\n",
    "                HDFS: Number of bytes read erasure-coded=0\n",
    "        Job Counters \n",
    "                Killed reduce tasks=1\n",
    "                Launched map tasks=5\n",
    "                Launched reduce tasks=3\n",
    "                Data-local map tasks=5\n",
    "                Total time spent by all maps in occupied slots (ms)=163929804\n",
    "                Total time spent by all reduces in occupied slots (ms)=97618380\n",
    "                Total time spent by all map tasks (ms)=48414\n",
    "                Total time spent by all reduce tasks (ms)=28830\n",
    "                Total vcore-milliseconds taken by all map tasks=48414\n",
    "                Total vcore-milliseconds taken by all reduce tasks=28830\n",
    "                Total megabyte-milliseconds taken by all map tasks=163929804\n",
    "                Total megabyte-milliseconds taken by all reduce tasks=97618380\n",
    "        Map-Reduce Framework\n",
    "                Map input records=35119\n",
    "                Map output records=219095\n",
    "                Map output bytes=2091576\n",
    "                Map output materialized bytes=596756\n",
    "                Input split bytes=655\n",
    "                Combine input records=219095\n",
    "                Combine output records=42051\n",
    "                Reduce input groups=29287\n",
    "                Reduce shuffle bytes=596756\n",
    "                Reduce input records=42051\n",
    "                Reduce output records=29287\n",
    "                Spilled Records=84102\n",
    "                Shuffled Maps =15\n",
    "                Failed Shuffles=0\n",
    "                Merged Map outputs=15\n",
    "                GC time elapsed (ms)=433\n",
    "                CPU time spent (ms)=13200\n",
    "                Physical memory (bytes) snapshot=4379766784\n",
    "                Virtual memory (bytes) snapshot=38611079168\n",
    "                Total committed heap usage (bytes)=4391436288\n",
    "                Peak Map Physical memory (bytes)=622166016\n",
    "                Peak Map Virtual memory (bytes)=4832083968\n",
    "                Peak Reduce Physical memory (bytes)=466513920\n",
    "                Peak Reduce Virtual memory (bytes)=4836503552\n",
    "        Shuffle Errors\n",
    "                BAD_ID=0\n",
    "                CONNECTION=0\n",
    "                IO_ERROR=0\n",
    "                WRONG_LENGTH=0\n",
    "                WRONG_MAP=0\n",
    "                WRONG_REDUCE=0\n",
    "        File Input Format Counters \n",
    "                Bytes Read=1341726\n",
    "        File Output Format Counters \n",
    "                Bytes Written=313829"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. [3 points] Run wordcount using the provided mapper_noll.py and the default\n",
    "reducer aggregate. The command is similar to\n",
    "\n",
    "    mapred streaming -file ~/big-data-repo/hadoop/mapper_noll.py -mapper mapper_noll.py \\\n",
    "-input /user/singhj/five-books -reducer aggregate \\\n",
    "-output /books-stream-count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran the command and this is the output:\n",
    "\n",
    "2024-10-13 22:32:23,447 INFO mapreduce.Job: Job job_1728858072744_0002 completed successfully\n",
    "2024-10-13 22:32:23,554 INFO mapreduce.Job: Counters: 54\n",
    "        File System Counters\n",
    "                FILE: Number of bytes read=660527\n",
    "                FILE: Number of bytes written=5387912\n",
    "                FILE: Number of read operations=0\n",
    "                FILE: Number of large read operations=0\n",
    "                FILE: Number of write operations=0\n",
    "                HDFS: Number of bytes read=1367597\n",
    "                HDFS: Number of bytes written=103696\n",
    "                HDFS: Number of read operations=48\n",
    "                HDFS: Number of large read operations=0\n",
    "                HDFS: Number of write operations=9\n",
    "                HDFS: Number of bytes read erasure-coded=0\n",
    "        Job Counters \n",
    "                Launched map tasks=11\n",
    "                Launched reduce tasks=3\n",
    "                Data-local map tasks=11\n",
    "                Total time spent by all maps in occupied slots (ms)=358150764\n",
    "                Total time spent by all reduces in occupied slots (ms)=93050666\n",
    "                Total time spent by all map tasks (ms)=105774\n",
    "                Total time spent by all reduce tasks (ms)=27481\n",
    "                Total vcore-milliseconds taken by all map tasks=105774\n",
    "                Total vcore-milliseconds taken by all reduce tasks=27481\n",
    "                Total megabyte-milliseconds taken by all map tasks=358150764\n",
    "                Total megabyte-milliseconds taken by all reduce tasks=93050666\n",
    "        Map-Reduce Framework\n",
    "                Map input records=35119\n",
    "                Map output records=207438\n",
    "                Map output bytes=4167234\n",
    "                Map output materialized bytes=660707\n",
    "                Input split bytes=1295\n",
    "                Combine input records=207438\n",
    "                Combine output records=26905\n",
    "                Reduce input groups=10201\n",
    "                Reduce shuffle bytes=660707\n",
    "                Reduce input records=26905\n",
    "                Reduce output records=10201\n",
    "                Spilled Records=53810\n",
    "                Shuffled Maps =33\n",
    "                Failed Shuffles=0\n",
    "                Merged Map outputs=33\n",
    "                GC time elapsed (ms)=878\n",
    "                CPU time spent (ms)=20770\n",
    "                Physical memory (bytes) snapshot=7693291520\n",
    "                Virtual memory (bytes) snapshot=67552075776\n",
    "                Total committed heap usage (bytes)=8057257984\n",
    "                Peak Map Physical memory (bytes)=643846144\n",
    "                Peak Map Virtual memory (bytes)=4834713600\n",
    "                Peak Reduce Physical memory (bytes)=429309952\n",
    "                Peak Reduce Virtual memory (bytes)=4830900224\n",
    "        Shuffle Errors\n",
    "                BAD_ID=0\n",
    "                CONNECTION=0\n",
    "                IO_ERROR=0\n",
    "                WRONG_LENGTH=0\n",
    "                WRONG_MAP=0\n",
    "                WRONG_REDUCE=0\n",
    "        File Input Format Counters \n",
    "                Bytes Read=1366302\n",
    "        File Output Format Counters \n",
    "                Bytes Written=103696\n",
    "2024-10-13 22:32:23,555 INFO streaming.StreamJob: Output directory: /books-stream-count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. [3 points] Run wordcount using the provided mapper_noll.py and the provided\n",
    "reducer reducer_noll.py. The command is similar to\n",
    "\n",
    "    mapred streaming -files ~/big-data-repo/hadoop/mapper_noll.py ~/big-data-repo/hadoop/reducer_noll.py \\\n",
    "-mapper mapper_noll.py \\\n",
    "-reducer reducer_noll.py \\\n",
    "-input /user/singhj/five-books \\\n",
    "-output /books-my-own-counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had to ran this command instead since the one in question 5 of this section didn't work due to an extra argument.\n",
    "\n",
    "This is the command I ran:\n",
    "\n",
    " mapred streaming -file ~/big-data-repo/hadoop/mapper_noll.py  -mapper mapper_noll.py                    -file ~/big-data-\n",
    "repo/hadoop/reducer_noll.py -reducer reducer_noll.py                  -input /\n",
    "user/adityad/five-books                  -output /books-my-own-counts\n",
    "\n",
    "This is the output:\n",
    "\n",
    "2024-10-13 22:43:07,569 INFO mapreduce.Job: Job job_1728858072744_0003 completed successfully\n",
    "2024-10-13 22:43:07,652 INFO mapreduce.Job: Counters: 55\n",
    "        File System Counters\n",
    "                FILE: Number of bytes read=4582128\n",
    "                FILE: Number of bytes written=13240172\n",
    "                FILE: Number of read operations=0\n",
    "                FILE: Number of large read operations=0\n",
    "                FILE: Number of write operations=0\n",
    "                HDFS: Number of bytes read=1367597\n",
    "                HDFS: Number of bytes written=236309\n",
    "                HDFS: Number of read operations=48\n",
    "                HDFS: Number of large read operations=0\n",
    "                HDFS: Number of write operations=9\n",
    "                HDFS: Number of bytes read erasure-coded=0\n",
    "        Job Counters \n",
    "                Killed reduce tasks=1\n",
    "                Launched map tasks=11\n",
    "                Launched reduce tasks=3\n",
    "                Data-local map tasks=11\n",
    "                Total time spent by all maps in occupied slots (ms)=358990492\n",
    "                Total time spent by all reduces in occupied slots (ms)=101329436\n",
    "                Total time spent by all map tasks (ms)=106022\n",
    "                Total time spent by all reduce tasks (ms)=29926\n",
    "                Total vcore-milliseconds taken by all map tasks=106022\n",
    "                Total vcore-milliseconds taken by all reduce tasks=29926\n",
    "                Total megabyte-milliseconds taken by all map tasks=358990492\n",
    "                Total megabyte-milliseconds taken by all reduce tasks=101329436\n",
    "        Map-Reduce Framework\n",
    "                Map input records=35119\n",
    "                Map output records=207438\n",
    "                Map output bytes=4167234\n",
    "                Map output materialized bytes=4582308\n",
    "                Input split bytes=1295\n",
    "                Combine input records=0\n",
    "                Combine output records=0\n",
    "                Reduce input groups=10201\n",
    "                Reduce shuffle bytes=4582308\n",
    "                Reduce input records=207438\n",
    "                Reduce output records=10201\n",
    "                Spilled Records=414876\n",
    "                Shuffled Maps =33\n",
    "                Failed Shuffles=0\n",
    "                Merged Map outputs=33\n",
    "                GC time elapsed (ms)=955\n",
    "                CPU time spent (ms)=21370\n",
    "                Physical memory (bytes) snapshot=7863332864\n",
    "                Virtual memory (bytes) snapshot=67558744064\n",
    "                Total committed heap usage (bytes)=8215592960\n",
    "                Peak Map Physical memory (bytes)=630722560\n",
    "                Peak Map Virtual memory (bytes)=4831543296\n",
    "                Peak Reduce Physical memory (bytes)=497270784\n",
    "                Peak Reduce Virtual memory (bytes)=4831469568\n",
    "        Shuffle Errors\n",
    "                BAD_ID=0\n",
    "                CONNECTION=0\n",
    "                IO_ERROR=0\n",
    "                WRONG_LENGTH=0\n",
    "                WRONG_MAP=0\n",
    "                WRONG_REDUCE=0\n",
    "        File Input Format Counters \n",
    "                Bytes Read=1366302\n",
    "        File Output Format Counters \n",
    "                Bytes Written=236309\n",
    "2024-10-13 22:43:07,652 INFO streaming.StreamJob: Output directory: /books-my-own-counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Analyzing Server Logs [55 points]\n",
    "\n",
    "A dataset representing Apache web server logs is available as access.log. Each row has the\n",
    "following schema:\n",
    "\n",
    "● IP of client: This refers to the IP address of the client that sent the request to the server.\n",
    "\n",
    "● Remote Log Name: Remote name of the User performing the request. In the majority of the\n",
    "applications, this is confidential information and is hidden or not available.\n",
    "\n",
    "● User ID: The ID of the user performing the request. In the majority of the applications, this is a\n",
    "piece of confidential information and is hidden or not available.\n",
    "\n",
    "● Date and Time: The date and time of the request are represented in UTC format as follows:\n",
    "\n",
    " -Day/Month/Year:Hour:Minutes: Seconds +Time-Zone-Correction.\n",
    "\n",
    "● Request Type: The type of request (GET, PUT, POST, etc.) that the server got. This depends on the\n",
    "operation that the request will do.\n",
    "\n",
    "● API: The API of the website to which the request is related. Example: When a user accesses a cart\n",
    "on a shopping website, the API comes as /usr/cart.\n",
    "\n",
    "● Protocol and Version: Protocol used for connecting with server and its version.\n",
    "\n",
    "● Status Code: Status code that the server returned for the request. Eg: 404 is sent when a requested\n",
    "resource is not found. 200 is sent when the request was successfully served. See the http status\n",
    "code registry listing for interpretations of status codes.\n",
    "\n",
    "● Byte: The amount of data in bytes that was sent back to the client.\n",
    "\n",
    "● Referrer: The websites/source from where the user was directed to the current website. If none, it\n",
    "is represented by “-“.\n",
    "\n",
    "● User Agent String: The user agent string contains details of the browser and the host device (like\n",
    "the name, version, device type etc.).\n",
    "\n",
    "● Response Time: The response time the server took to serve the request. This is the difference\n",
    "between the timestamps when the request was received and when the request was served.\n",
    "\n",
    "Use Hadoop to perform analytics on the provided data3.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [6+9=15 points4] What is the percentage of each request type (GET, PUT, POST, etc.)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I first figured out how many request types there are. Then I wanted to know what those request types are. Finally, I wanted to find the unique count of each request type.\n",
    "\n",
    "I did these commands:\n",
    "\n",
    "\n",
    "aditya_duggirala@cluster-a30e-m:~/big-data-repo/datasets$ grep -oP '\"(GET|POST|PUT|DELETE|HEAD|OPTIONS|PATCH) ' access.log | sort | uniq | wc -l\n",
    "\n",
    "3\n",
    "\n",
    "\n",
    "\n",
    "aditya_duggirala@cluster-a30e-m:~/big-data-repo/datasets$ grep -oP '\"(GET|POST|PUT|DELETE|HEAD|OPTIONS|PATCH) ' access.log | sort | uniq\n",
    "\n",
    "\"GET \n",
    "\n",
    "\"HEAD \n",
    "\n",
    "\"POST \n",
    "\n",
    "aditya_duggirala@cluster-a30e-m:~/big-data-repo/datasets$ grep -oP '\"(GET|POST|PUT|DELETE|HEAD|OPTIONS|PATCH) ' access.log | sort | uniq -c\n",
    "\n",
    "  33414 \"GET \n",
    "\n",
    "  253 \"HEAD \n",
    "\n",
    "  44584 \"POST \n",
    "\n",
    "  The percentages are: \n",
    "\n",
    "  GET: 42.71%\n",
    "\n",
    "  HEAD: 0.32%\n",
    "    \n",
    "  POST: 56.98%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. [6+9=15 points] What percent of the responses fall into each of the following five types?\n",
    "\n",
    "    ● Informational responses (100–199)\n",
    "\n",
    "    ● Successful responses (200–299)\n",
    "\n",
    "    ● Redirection messages (300–399)\n",
    "\n",
    "    ● Client error responses (400–499)\n",
    "\n",
    "    ● Server error responses (500–599)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used awk to process each line of text in the access log and print the 9th field which is the status code paired with grep to search for extended regular expression pattern of 100-199 200-299 and etc. Finally, I got the line count of each status code.\n",
    "\n",
    "\n",
    "I did these commands:\n",
    "\n",
    "aditya_duggirala@cluster-a30e-m:~/big-data-repo/datasets$ \n",
    "\n",
    "#Count informational responses (100–199)\n",
    "awk '{print $9}' access.log | grep -E '1[0-9][0-9]' | wc -l\n",
    "\n",
    "#Count successful responses (200–299)\n",
    "awk '{print $9}' access.log | grep -E '2[0-9][0-9]' | wc -l\n",
    "\n",
    "#Count redirection messages (300–399)\n",
    "awk '{print $9}' access.log | grep -E '3[0-9][0-9]' | wc -l\n",
    "\n",
    "#Count client error responses (400–499)\n",
    "awk '{print $9}' access.log | grep -E '4[0-9][0-9]' | wc -l\n",
    "\n",
    "#Count server error responses (500–599)\n",
    "awk '{print $9}' access.log | grep -E '5[0-9][0-9]' | wc -l\n",
    "\n",
    "0\n",
    "\n",
    "70684\n",
    "\n",
    "2929\n",
    "\n",
    "4638\n",
    "\n",
    "0\n",
    "\n",
    "The percentages are: \n",
    "\n",
    "(100–199): 0%\n",
    "\n",
    "(200–299): 90.33%\n",
    "\n",
    "(300–399): 3.74%\n",
    "\n",
    "(400–499): 5.93%\n",
    "\n",
    "(500–599): 0%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. [9+16=25 points] What 5 IP addresses generate the most client errors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used awk to extract the 1st field and 9th field to get the IP-Addresses and status code and then filter the lines to include the client error status codes 400-499 and then extract the ip-addresses with those codes. Finally I sorted the top five unique counted ip address from the highest count to lowest count of client errors\n",
    "\n",
    "\n",
    "I used this command:\n",
    "\n",
    "awk print $1,$9  access.log | grep ' 4[0-9][0-9]$' | awk '{print $1}' | sort | uniq -c | sort -nr | head -n 5\n",
    "   \n",
    "   2059 173.255.176.5\n",
    "\n",
    "   126 212.9.160.24\n",
    "\n",
    "   78 13.77.204.88\n",
    "\n",
    "   58 51.210.243.185\n",
    "     \n",
    "   53 193.106.30.100\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Presidential Speeches [15 points]\n",
    "All US presidential speeches are available as a single zip in J’s Github repo.\n",
    "Each speech may be cleaned with this filter:\n",
    "\n",
    "import requests\n",
    "\n",
    "import re\n",
    "\n",
    "import string\n",
    "\n",
    "stopwords_list =\n",
    "requests.get(\"https://gist.githubusercontent.com/rg089/35e00abf8941d72d419224cfd5b5925d\n",
    "/raw/12d899b70156fd0041fa9778d657330b024b959c/stopwords.txt\").content\n",
    "stopwords = list(set(stopwords_list.decode().splitlines()))\n",
    "\n",
    "def remove_stopwords(words):\n",
    "list_ = re.sub(r\"[^a-zA-Z0-9]\", \" \", words.lower()).split())\n",
    "return [itm for itm in list_ if itm not in stopwords]\n",
    "\n",
    "def clean_text(text):\n",
    "text = text.lower()\n",
    "text = re.sub('\\[.*?\\]', '', text)\n",
    "text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "text = re.sub('[\\d\\n]', ' ', text)\n",
    "return ' '.join(remove_stopwords(text))\n",
    "\n",
    "The goal of this analysis is to calculate the sentiment of each president’s speeches. One way to\n",
    "compute the sentiment of a collection of words is to take the average of their valences.5 The\n",
    "AFINN-165 collection contains the valences of 3,382 English words.\n",
    "\n",
    "Write a function valence (text) such that it takes a line of any presidential speech and returns\n",
    "its valence after cleaning it. It should be a functional program, conforming to the pattern:\n",
    "\n",
    "def valence(text):\n",
    "\n",
    "return calc_valence(clean_text(text))\n",
    "\n",
    "where calc_valence(text) is a function that you write. Be sure to test this function under any\n",
    "imaginable conditions, for example:\n",
    "\n",
    "● When text is empty,\n",
    "\n",
    "● When text is a string of non-printable characters,\n",
    "\n",
    "● When text is a bytecode string,\n",
    "\n",
    "The function must be in a form that we can use for testing in our environment against data that\n",
    "you don’t have access to. The presidential speeches should be considered a representative\n",
    "sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the average valence of each president’s speeches according to this outline:\n",
    "1. [7 points] In the mapper (which is given a sequence of lines of speeches as input):\n",
    "    a. Clean each line as suggested above,\n",
    "\n",
    "    b. Calculate the valence of each word in the line,\n",
    "\n",
    "    c. Emit a (tab-separated) key-value pair (president, word valence) for each word in the line.\n",
    "2. [6 points] In the reducer (which is given all (president, word valence) key-value pairs\n",
    "with the same key, i.e.president):\n",
    "\n",
    "    a. Compute the average valence of all words spoken by the president,\n",
    "\n",
    "    b. Emit a (tab-separated) key-value pair (president, sentiment of president’s\n",
    "speeches)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both of these questions I have a txt files for these that I will include in the zip folder of quiz 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "Sample of AFINN words and their valence scores: [('abandon', -2), ('abandoned', -2), ('abandons', -2), ('abducted', -2), ('abduction', -2), ('abductions', -2), ('abhor', -3), ('abhorred', -3), ('abhorrent', -3), ('abhors', -3)]\n",
      "3.0\n",
      "-2.6666666666666665\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env \n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "\n",
    "# Download stopwords list\n",
    "stopwords_url = \"https://gist.githubusercontent.com/rg089/35e00abf8941d72d419224cfd5b5925d/raw/12d899b70156fd0041fa9778d657330b024b959c/stopwords.txt\"\n",
    "stopwords_list = requests.get(stopwords_url).content\n",
    "stopwords = set(stopwords_list.decode().splitlines())\n",
    "\n",
    "# Function to remove stopwords and clean text\n",
    "def remove_stopwords(words):\n",
    "    list_ = re.sub(r\"[^a-zA-Z0-9]\", \" \", words.lower()).split()  # Remove non-alphanumeric characters\n",
    "    return [itm for itm in list_ if itm not in stopwords]\n",
    "\n",
    "# Function to clean text and remove stopwords\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  # Remove anything in brackets\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', text)  # Remove punctuation\n",
    "    text = re.sub(r'[\\d\\n]', ' ', text)  # Remove digits and newlines\n",
    "    return ' '.join(remove_stopwords(text))\n",
    "\n",
    "\n",
    "# Load the AFINN wordlist (replace the path with your correct path)\n",
    "afinn = {}\n",
    "afinn_path = \"C:\\\\Users\\\\adity\\\\OneDrive\\\\Tufts University Online MS CS\\\\2024\\\\Fall Sem\\\\CS 119\\\\Quizzes\\\\Quiz4\\\\AFINN-en-165.txt\"\n",
    "\n",
    "with open(afinn_path, 'r') as file:\n",
    "    for line in file:\n",
    "        word, score = line.split('\\t')\n",
    "        afinn[word] = int(score)\n",
    "\n",
    "# Function to calculate the valence\n",
    "def calc_valence(text):\n",
    "    if isinstance(text, bytes):\n",
    "        text = text.decode(\"utf-8\", errors=\"ignore\")\n",
    "    \n",
    "    if not text.strip():\n",
    "        return 0\n",
    "    \n",
    "    words = text.split()\n",
    "    valence_sum = 0\n",
    "    word_count = 0\n",
    "\n",
    "    for word in words:\n",
    "        if word in afinn:\n",
    "            valence_sum += afinn[word]\n",
    "            word_count += 1\n",
    "\n",
    "    return valence_sum / word_count if word_count > 0 else 0\n",
    "\n",
    "# Main valence function\n",
    "def valence(text):\n",
    "    if isinstance(text, bytes):  # Convert bytecode to string\n",
    "        text = text.decode(\"utf-8\", errors=\"ignore\")\n",
    "    cleaned_text = clean_text(text)\n",
    "    return calc_valence(cleaned_text)\n",
    "\n",
    "# Testing the valence function\n",
    "print(valence(\"\"))  # Edge case: empty text\n",
    "print(valence(\"!@#$%\"))  # Edge case: special characters only\n",
    "print(valence(b\"\\x80\\x81\\x82\"))  # Edge case: bytecode strings\n",
    "\n",
    "# Print a sample of AFINN words and their valence scores\n",
    "print(\"Sample of AFINN words and their valence scores:\", list(afinn.items())[:10])\n",
    "\n",
    "print(valence(\"happy joy love\"))  # Should return a positive valence\n",
    "print(valence(\"sad anger hate\"))  # Should return a negative valence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2 points] How much data, in bytes, was emitted by the mappers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map-Reduce Framework\n",
    "\n",
    "Map input records=40232\n",
    "\n",
    "Map output records=39210\n",
    "\n",
    "Map output bytes=596825\n",
    "\n",
    "596,825 bytes were emitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Hadoop Errors [15 points]\n",
    "When dealing with errors in Hadoop, where the execution is distributed to hundreds of workers,\n",
    "an error message could end up in a log file on any of those servers. This is a scavenger hunt\n",
    "question. We deliberately modify the code so it would occasionally fail and look for the error\n",
    "message so we can find them! The provided mapper for Hadoop Streaming, mapper_noll.py,\n",
    "with the changed lines shown in red7.\n",
    "Run Hadoop Streaming on the five books we have been using for practice, using this modified\n",
    "mapper. It will fail, of course!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[7 points] Where (what server & location) did the divide-by-zero error messages show up and\n",
    "how many did you find?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[8 points] How many such messages did you find? Is the count you found consistent with what\n",
    "you might expect from random.randint(0,99)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explainations for both questions below:\n",
    "\n",
    "\n",
    "## Start Hadoop Streaming Job:\n",
    "\n",
    "First, I modified the mapper_noll.py file by uncommenting the line responsible for generating the divide-by-zero error:\n",
    "\n",
    "\n",
    "nano ~/big-data-repo/hadoop/mapper_noll.py\n",
    "I uncommented the line:\n",
    "\n",
    "\n",
    "\n",
    "x = 1 / random.randint(0,99)\n",
    "Then, I used the following command to run the Hadoop Streaming job:\n",
    "\n",
    "\n",
    "\n",
    "mapred streaming -file ~/big-data-repo/hadoop/mapper_noll.py -mapper mapper_noll.py \\\n",
    "                 -input /user/adityad/five-books -reducer\n",
    "                 -output /hadoop-error-test\n",
    "The job failed due to the divide-by-zero error.\n",
    "\n",
    "\n",
    "## Find the Application ID:\n",
    "\n",
    "To diagnose the issue, I listed all YARN applications to find the application ID of the failed job:\n",
    "bash\n",
    "\n",
    "yarn application -list -appStates ALL\n",
    "\n",
    "I found the application ID as application_1728922376681_0001, which I would use in subsequent steps.\n",
    "Extract Logs for the Application:\n",
    "\n",
    "Using the application ID, I fetched the logs to search for divide-by-zero errors:\n",
    "\n",
    "\n",
    "yarn logs -applicationId application_1728922376681_0001 | grep \"ZeroDivisionError\"\n",
    "\n",
    "Since the logs didn't explicitly mention \"ZeroDivisionError,\" \n",
    "\n",
    "I looked for other related errors, specifically checking for failures due to subprocess failed with code 1:\n",
    "\n",
    "yarn logs -applicationId application_1728922376681_0001 | grep \"subprocess failed with code 1\"\n",
    "\n",
    "## Server and Location:\n",
    "\n",
    "The server responsible for processing this job is cluster-a30e-m.c.united-bot-438500-e7.internal, as shown in the logs:\n",
    "\n",
    "Connecting to ResourceManager at cluster-a30e-m.c.united-bot-438500-e7.internal./10.128.0.2:8032\n",
    "\n",
    "The location where the divide-by-zero errors occurred is within the tasks executed on this Hadoop cluster, as evidenced by the repeated task failure reports in the logs.\n",
    "\n",
    "## Count of IPC Server Handlers:\n",
    "\n",
    "I counted how many times the divide-by-zero error occurred by looking for IPC Server handler messages, which are involved when tasks fail due to the mapper's error:\n",
    "\n",
    "yarn logs -applicationId application_1728922376681_0001 | grep \"IPC Server handler\" | wc -l\n",
    "\n",
    "This command returned 90 IPC handler logs indicating task failures due to the divide-by-zero error.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Expected Outcome Based on random.randint(0,99) Logic:\n",
    "\n",
    "The code random.randint(0,99) has a 1% chance of generating a zero, which triggers the divide-by-zero error. Given that the job launched many tasks and each task processes multiple lines of input, having 90 errors is consistent with this probability.\n",
    "Since the job involved launching multiple map tasks, with each task processing a portion of the input data, the total number of errors (90) falls within the expected range for a 1% error rate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml135_env_sp21",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
